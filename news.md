---
layout: inner
title: News
permalink: /news/
---

<!-- Text can be **bold**, _italic_, ~~strikethrough~~ or `keyword`.

[Link to another page](/index.html).

There should be whitespace between paragraphs. -->

# News

Followings are some recent news about me, important or miscellaneous.

## 23.08 Talk at WYMK 2023

I will give an invited talk at <a href="https://sites.google.com/view/wymk2023">Workshop for Young Mathematicians in Korea (WYMK 2023)</a> on sampling algorithms and diffusion models.

## 23.07 New paper on diffusion models with KRAFTON

Our new paper <q><a href="https://arxiv.org/abs/2307.02770">Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback</a></q> has been announced.

This is a joint work with KRAFTON AI Research Center. The paper proposes a methodology to align diffusion models' sample generation with specified practical requirements, and notably, it is the most empirical paper of mine.

## 23.06 New ICML Workshop paper

Our paper <q><a href="">Diffusion Probabilistic Models Generalize when They Fail to Memorize</a></q> has been accepted to <a href="https://spigmworkshop.github.io/">ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling</a>.

## 23.06 Talk at SIAM OP23

I will give a talk at 2023 SIAM Conference on Optimization on my paper <q><a href="https://arxiv.org/abs/2205.11093">Accelerated Minimax Algorithms Flock Together</a></q>. 

## 22.10 Talk at INFORMS

I will give a talk at 2022 INFORMS Annual Meeting on my papers on minimax optimization.

## 22.07 Youlchon Fellowship

I have been selected as a recipient of <a href="https://aiis.snu.ac.kr/bbs/board.php?bo_table=sub4_3">Youlchon AI Star Fellowship</a>. Many thanks!

## 22.05 New paper on minimax optimization

Our new paper <q><a href="https://arxiv.org/abs/2205.11093">Accelerated Minimax Algorithms Flock Together</a></q> has been announced. 

It establishes a new connection, the **<em>merging path property</em>**, among order-optimal algorithms for minimax optimization and order-optimal algorithms for fixed-point problems, which are all based on the mechanism of anchoring ($\approx$ Halpern iteration).

The paper is currently under revision at <i>SIAM Journal on Optimization</i>.

## 22.02 AISTATS paper

Our paper <q><a href="https://proceedings.mlr.press/v151/yoon22a.html">Robust Probabilistic Time Series Forecasting</a></q> has been accepted for publication at AISTATS 2022.

This work is based on my internship project at AWS AI Labs, where I worked with <a href="https://youngsuk0723.github.io/">Youngsuk Park</a> and <a href="http://web.mit.edu/~ywang02/www/">Bernie Wang</a>.

## 21.05 ~ 21.08 Internship: AWS AI Labs 

I will be working at <a href="https://aws.amazon.com/ai/?nc1=h_ls">AWS AI Labs</a> as an applied scientist intern.

## 21.02 ICML Papers

Two papers have been accepted for publication at ICML 2021.

<q><a href="http://proceedings.mlr.press/v139/yoon21d.html">Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with $\mathcal{O}(1/k^2)$ Rate on Squared Gradient Norm</a></q> is selected as a long talk _(top 166/5513=3% of papers)_.<br>
It introduces the algorithm **_Extra Anchored Gradient_** which reduces the gradient norm of smooth convex-concave objectives with an optimal complexity.

<q><a href="http://proceedings.mlr.press/v139/no21a.html">WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points</a></q> analyzes setups in which a WGAN objective is favorable to training via alternating gradient ascent-descent.
