---
layout: inner
title: News
permalink: /news/
---

<!-- Text can be **bold**, _italic_, ~~strikethrough~~ or `keyword`.

[Link to another page](/index.html).

There should be whitespace between paragraphs. -->

# News

Followings are some recent news of mine, important or miscellaneous.

## 22.07 Youlchon Fellowship

I have been selected as a recipient of <a href="https://aiis.snu.ac.kr/bbs/board.php?bo_table=sub4_3">Youlchon AI Star Fellowship</a>. Many thanks!

## 22.05 New paper on minimax optimization

Our new paper <q><a href="https://arxiv.org/abs/2205.11093">Accelerated Minimax Algorithms Flock Together</a></q> has been announced. 

It establishes a new connection, the **<em>merging path property</em>**, among order-optimal algorithms for minimax optimization and order-optimal algorithms for fixed-point problems, which are all based on the mechanism of anchoring ($\approx$ Halpern iteration).

## 22.02 AISTATS paper

Our paper <q><a href="https://proceedings.mlr.press/v151/yoon22a.html">Robust Probabilistic Time Series Forecasting</a></q> has been accepted for publication at AISTATS 2022.

This work is based on my internship project at AWS AI Labs, where I worked with <a href="https://youngsuk0723.github.io/">Youngsuk Park</a> and <a href="http://web.mit.edu/~ywang02/www/">Bernie Wang</a>.

## 21.05 ~ 21.08 Internship: AWS AI Labs 

I will be working at <a href="https://aws.amazon.com/ai/?nc1=h_ls">AWS AI Labs</a> as an applied scientist intern.

## 21.02 ICML Papers

Two papers have been accepted for publication at ICML 2021.

<q><a href="http://proceedings.mlr.press/v139/yoon21d.html">Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with $\mathcal{O}(1/k^2)$ Rate on Squared Gradient Norm</a></q> is selected as a long talk _(top 166/5513=3% of papers)_.<br>
It introduces the algorithm **_Extra Anchored Gradient_** which reduces the gradient norm of smooth convex-concave objectives with an optimal complexity.

<q><a href="http://proceedings.mlr.press/v139/no21a.html">WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points</a></q> analyzes setups in which a WGAN objective is favorable to training via alternating gradient ascent-descent.
